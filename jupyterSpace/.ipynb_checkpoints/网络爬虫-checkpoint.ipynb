{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络爬虫\n",
    "\n",
    "编码：utf-8；作者：王明智；Email：1765471602@qq.com；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入第三方库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import geopandas\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from matplotlib import colors\n",
    "from wordcloud import WordCloud\n",
    "from shapely.geometry import MultiPolygon, Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimSun']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取交叉口经纬度数据\n",
    "\n",
    "根据交叉口名称，通过高德地图API接口爬取交叉口的经纬度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getintersectionlatlng(temp_crossroadname, result, error):\n",
    "    try:\n",
    "        url_1 = 'https://restapi.amap.com/v3/geocode/geo?address='\n",
    "        url_2 = '&batch=true&output=json&key=eff48ee434d763609e59839fa946b9e1'\n",
    "        url = url_1 + '|'.join(temp_crossroadname) + url_2  # 对把交叉口名包含在url中\n",
    "\n",
    "        r_text = requests.get(url)\n",
    "        r_text.raise_for_status()  # 当出现错误时及时抛出错误\n",
    "        content = json.loads(r_text.content)\n",
    "        r_text.close()  # 很重要的一步！！！，否则会导致错误\n",
    "\n",
    "        status = content[\"status\"]\n",
    "        for k in range(int(content[\"count\"])):\n",
    "            if status == \"1\":\n",
    "                adcode = content[\"geocodes\"][k][\"adcode\"]\n",
    "                formatted_address = content[\"geocodes\"][k][\"formatted_address\"]\n",
    "                location = content[\"geocodes\"][k][\"location\"]\n",
    "                level = content[\"geocodes\"][k][\"level\"]\n",
    "                result.append((temp_crossroadname[k], formatted_address, adcode, location, level))\n",
    "            else:\n",
    "                error.append(temp_crossroadname[k])\n",
    "                print('error!')\n",
    "    except TimeoutError:\n",
    "        print('timeout error')\n",
    "\n",
    "\n",
    "result = []  # 设置一个列表用来存放提取结果\n",
    "error = []  # 设置一个列表用来存放请求失败的交叉口数据\n",
    "result.append(('Name', 'formatted_address', 'adcode', 'location', 'level'))\n",
    "error.append('Name')\n",
    "\n",
    "with open(r'F:\\18120900\\桌面\\地理逆编码.txt', 'r', encoding='utf-8') as f:\n",
    "    crossRoad = f.readlines()\n",
    "print(len(crossRoad))\n",
    "\n",
    "temp_crossRoadName = []  # 设置一个列表用来存放交叉口名称\n",
    "i = -1\n",
    "for crossRoadName in crossRoad:\n",
    "    i += 1\n",
    "    temp_crossRoadName.append(crossRoadName.replace('\\n', ''))\n",
    "    if i % 10 == 9 or i == len(crossRoad) - 1:\n",
    "        print(i + 1)  # 显示处理到那一个交叉口了\n",
    "        getintersectionlatlng(temp_crossRoadName, result, error)\n",
    "        temp_crossRoadName.clear()\n",
    "        time.sleep(2)\n",
    "df = pd.DataFrame(result)\n",
    "df.to_excel(r'F:\\18120900\\桌面\\地理逆编码处理结果.xlsx')\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取省市县区划Shape数据\n",
    "\n",
    "参考连接：https://mp.weixin.qq.com/s/cUW7cm0_shipSs2_-3x5Ag\n",
    "参考连接：https://mp.weixin.qq.com/s/JKP-Do8zR_hiW4qJrahgYQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(_columns, _properties, _geometry, url, _code):\n",
    "    \"\"\"\n",
    "    爬取url数据并返回_columns, _properties, _geometry\n",
    "    @param _columns: 用来存放列名\n",
    "    @param _geometry: 用来存放每个区划的polygon\n",
    "    @param _properties: 每个区划的相关信息\n",
    "    @param url: url\n",
    "    @param _code: 待爬取城市的代码，与url拼合后爬取数据\n",
    "    @return: _columns, _properties, _geometry\n",
    "    \"\"\"\n",
    "    r_text = requests.get(url + _code + '.json')\n",
    "    r_text.raise_for_status()  # 当出现错误时及时抛出错误\n",
    "    content = json.loads(r_text.content)  # 解析url返回的数据\n",
    "    for item in content['features']:\n",
    "        if not _columns:\n",
    "            _columns.append(list(item['properties'].keys()))\n",
    "        # 获取对应值,并把列表中的值全部转为str否则生成shape文件时会存在问题\n",
    "        _properties.append([str(item) for item in list(item['properties'].values())])\n",
    "        polygons = [Polygon(coordinate[0]) for coordinate in item['geometry']['coordinates']]\n",
    "        _geometry.append(MultiPolygon(polygons))\n",
    "\n",
    "\n",
    "def draw(data):\n",
    "    fig, ax = plt.subplots()\n",
    "    data.to_crs({'init': 'epsg:4524'}).plot(ax=ax, alpha=0.85)  # 投影到epsg:4524,避免看起来扁\n",
    "    plt.title(\"中国地图\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def geojson2shape(data, file_save, crs):\n",
    "    \"\"\"\n",
    "    geojson文件转存为shape文件\n",
    "    @param data: GeoDataFrame格式数据\n",
    "    @param file_save: shape文件保存地址\n",
    "    @param crs: 指定shape文件的坐标系统\n",
    "    @return: None\n",
    "    \"\"\"\n",
    "    data.to_crs(crs, inplace=True)\n",
    "    data.to_file(file_save + '.shp', driver='ESRI Shapefile', encoding='utf-8')\n",
    "    print(\"保存成功，文件存放在：\" + file_save)\n",
    "\n",
    "\n",
    "def geojson2file(data, file_save, crs):\n",
    "    data.to_crs(crs, inplace=True)\n",
    "    data.to_json(file_save + '.json')\n",
    "    print(\"保存成功，文件存放在：\" + file_save)\n",
    "\n",
    "\n",
    "columns, properties, geometry = [], [], []\n",
    "areacode_list = ['410000_full']\n",
    "for code in areacode_list:\n",
    "    crawler(columns, properties, geometry, r\"https://geo.datav.aliyun.com/areas_v2/bound/\", code)\n",
    "df = pd.DataFrame(properties, columns=columns[0])\n",
    "gdf = geopandas.GeoDataFrame(df, geometry=geometry)\n",
    "gdf.crs = {'init': 'epsg:4326'}  # 设置geojson的地理坐标系\n",
    "draw(gdf)                        # 画出geojson地图\n",
    "# file_name = '省级行政区划'\n",
    "# geojson2shape(gdf, file_name, {'init': 'epsg:4326'})  # 转存为shape文件\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取高德地图收费站数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getstationinfo(url, params):\n",
    "    \"\"\"\n",
    "    @param url: str\n",
    "    @param params: dict\n",
    "    @return: Dataframe\n",
    "    \"\"\"\n",
    "    js = crawler(url, params)\n",
    "    name = [key for key, value in js['pois'][0].items() if isinstance(value, str)]\n",
    "    pagecount = -(-int(js['count']) // params['offset'])\n",
    "    print('total station count:', js['count'])\n",
    "    temp_list = []\n",
    "    for i in range(pagecount):\n",
    "        params['page'] = i + 1\n",
    "        js = crawler(url, params)\n",
    "        if js['status'] == '1':\n",
    "            for v in js['pois']:\n",
    "                temp_list.append([value for value in v.values() if isinstance(value, str)])\n",
    "        else:\n",
    "            print('meet an error in page:', params['page'])\n",
    "            continue\n",
    "        print('processing:', (params['page'] - 1) * params['offset'] + len(js['pois']))\n",
    "    return pd.DataFrame(columns=name, data=temp_list)\n",
    "\n",
    "\n",
    "def crawler(url, params):\n",
    "    requests.DEFAULT_RETRIES = 5\n",
    "    res = requests.get(url, params, timeout=25)\n",
    "    res.raise_for_status()  # 如果响应状态码不是 200，就主动抛出异常\n",
    "    js = json.loads(res.text)\n",
    "    res.close()\n",
    "    time.sleep(5)\n",
    "    return js\n",
    "\n",
    "\n",
    "def GCJ2WGS(location):\n",
    "    \"\"\"\n",
    "    @param location: locations[1] = \"113.923745,22.530824\"\n",
    "    @return: str: wgsLon,wgsLat\n",
    "    \"\"\"\n",
    "    # 官方API: http://lbs.amap.com/api/webservice/guide/api/convert\n",
    "    # 坐标体系说明：http://lbs.amap.com/faq/top/coordinate/3\n",
    "    # GCJ02->WGS84 Java版本：http://www.cnblogs.com/xinghuangroup/p/5787306.html\n",
    "    # 验证坐标转换正确性的地址：http://www.gpsspg.com/maps.htm\n",
    "\n",
    "    lon = float(location[0:location.find(\",\")])\n",
    "    lat = float(location[location.find(\",\") + 1:len(location)])\n",
    "    a = 6378245.0  # 克拉索夫斯基椭球参数长半轴a\n",
    "    ee = 0.00669342162296594323  # 克拉索夫斯基椭球参数第一偏心率平方\n",
    "    PI = 3.14159265358979324  # 圆周率\n",
    "    # 以下为转换公式\n",
    "    x = lon - 105.0\n",
    "    y = lat - 35.0\n",
    "    # 经度\n",
    "    dLon = 300.0 + x + 2.0 * y + 0.1 * x * x + 0.1 * x * y + 0.1 * math.sqrt(abs(x))\n",
    "    dLon += (20.0 * math.sin(6.0 * x * PI) + 20.0 * math.sin(2.0 * x * PI)) * 2.0 / 3.0\n",
    "    dLon += (20.0 * math.sin(x * PI) + 40.0 * math.sin(x / 3.0 * PI)) * 2.0 / 3.0\n",
    "    dLon += (150.0 * math.sin(x / 12.0 * PI) + 300.0 * math.sin(x / 30.0 * PI)) * 2.0 / 3.0\n",
    "    # 纬度\n",
    "    dLat = -100.0 + 2.0 * x + 3.0 * y + 0.2 * y * y + 0.1 * x * y + 0.2 * math.sqrt(abs(x))\n",
    "    dLat += (20.0 * math.sin(6.0 * x * PI) + 20.0 * math.sin(2.0 * x * PI)) * 2.0 / 3.0\n",
    "    dLat += (20.0 * math.sin(y * PI) + 40.0 * math.sin(y / 3.0 * PI)) * 2.0 / 3.0\n",
    "    dLat += (160.0 * math.sin(y / 12.0 * PI) + 320 * math.sin(y * PI / 30.0)) * 2.0 / 3.0\n",
    "    radLat = lat / 180.0 * PI\n",
    "    magic = math.sin(radLat)\n",
    "    magic = 1 - ee * magic * magic\n",
    "    sqrtMagic = math.sqrt(magic)\n",
    "    dLat = (dLat * 180.0) / ((a * (1 - ee)) / (magic * sqrtMagic) * PI)\n",
    "    dLon = (dLon * 180.0) / (a / sqrtMagic * math.cos(radLat) * PI)\n",
    "    wgsLon = lon - dLon\n",
    "    wgsLat = lat - dLat\n",
    "    return str(round(wgsLon, 6)) + ',' + str(round(wgsLat, 6))\n",
    "\n",
    "\n",
    "url = 'https://restapi.amap.com/v3/place/text?'\n",
    "params = {'keywords': '收费站', 'city': 'heilongjiang', 'page': '1', 'offset': 20, 'key': 'eff48ee434d763609e59839fa946b9e1'}\n",
    "df = getstationinfo(url, params)\n",
    "\n",
    "df['wgsLocation'] = df['location'].map(GCJ2WGS)\n",
    "time = time.strftime('%Y.%m.%d',time.localtime(time.time()))\n",
    "save_path = 'F:\\\\18120900\\\\桌面\\\\%s-%s.xlsx'%(params['city'], time)\n",
    "df.to_excel(save_path, index=None)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取OSM数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler():\n",
    "    city = ox.graph_from_place(\"福田区,深圳\")  # 从OSM上爬取福田区地图\n",
    "    ox.plot_graph(city)  # 用python展示地图\n",
    "    ox.save_graph_shapefile(city, filepath='szft')  # 保存地图\n",
    "\n",
    "\n",
    "crawler()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取豆瓣数据并生成词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [\n",
    "    '#CD853F', '#DC143C', '#00FF7F', '#FF6347', '#8B008B', '#00FFFF',\n",
    "    '#0000FF', '#8B0000', '#FF8C00', '#1E90FF', '#00FF00', '#FFD700',\n",
    "    '#008080', '#008B8B', '#8A2BE2', '#228B22', '#FA8072', '#808080']\n",
    "\n",
    "\n",
    "def getCommentsFromDouban(url, headers, comments):\n",
    "    try:\n",
    "        r_text = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(r_text.text, 'lxml')\n",
    "        pattern = soup.find_all('span', 'short')\n",
    "        for item in pattern:\n",
    "            comments.append(item.string)\n",
    "\n",
    "        pattern_s = re.compile('<span class=\"user-stars allstar(.*?) rating\"')\n",
    "        p = re.findall(pattern_s, r_text.text)\n",
    "        s = 0\n",
    "        for star in p:\n",
    "            s += int(star)\n",
    "        print(s)\n",
    "\n",
    "    except TimeoutError:\n",
    "        print('Unknow error')\n",
    "\n",
    "\n",
    "def simpleWC3(sep=' ', back='black', freDictpath='data_fre.json', savepath='res.png'):\n",
    "    \"\"\"\n",
    "    词云可视化Demo【自定义字体的颜色】\n",
    "    \"\"\"\n",
    "    # 基于自定义颜色表构建colormap对象\n",
    "    colormap = colors.ListedColormap(color_list)\n",
    "    try:\n",
    "        with open(freDictpath) as f:\n",
    "            data = f.readlines()\n",
    "            data_list = [one.strip().split(sep) for one in data if one]\n",
    "        fre_dict = {}\n",
    "        for one_list in data_list:\n",
    "            fre_dict[one_list[0]] = int(one_list[1])\n",
    "    except FileNotFoundError:\n",
    "        fre_dict = freDictpath\n",
    "    wc = WordCloud(\n",
    "        font_path='font/simhei.ttf',  # 设置字体  #simhei\n",
    "        background_color=back,  # 背景颜色\n",
    "        max_words=2300,  # 词云显示的最大词数\n",
    "        max_font_size=120,  # 字体最大值\n",
    "        colormap=colormap,  # 自定义构建colormap对象\n",
    "        margin=2, width=3200, height=2400, random_state=42, prefer_horizontal=0.5)  # 无法水平放置就垂直放置\n",
    "    wc.generate_from_frequencies(fre_dict)\n",
    "    plt.figure()\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    # wc.to_file(savepath)\n",
    "\n",
    "\n",
    "# 获取书评内容\n",
    "pageNum = 10\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0;Win64;64) AppleWebKit/537.36(KHTML,like Gecko) Chrome/78.03904.108 Safari/537.36'}\n",
    "comments = []\n",
    "for index in range(1, pageNum + 1):\n",
    "    url = 'https://book.douban.com/subject/1029553/comments/hot?p=' + str(index)\n",
    "    getCommentsFromDouban(url, headers, comments)\n",
    "\n",
    "# 生成词云\n",
    "word_list = \"/\".join(jieba.cut('。'.join(comments))).split('/')\n",
    "fre_dict = {}\n",
    "for one in word_list:\n",
    "    if one in fre_dict:\n",
    "        fre_dict[one] += 1\n",
    "    else:\n",
    "        fre_dict[one] = 1\n",
    "simpleWC3(sep=' ', back='black', freDictpath=fre_dict, savepath='simpleWC3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取疫情数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2json(str, code):\n",
    "    return str.replace('\\\\', '').replace('(', '').replace(')', '').replace(code, '').replace('\"{', '{').replace('}\"', '}')\n",
    "\n",
    "\n",
    "def get_json(url, code):\n",
    "    response = str(requests.get(url).content, 'utf-8')\n",
    "    res = str2json(response, code)\n",
    "    data = json.loads(res)  # 提取数据部分\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_china_data(data):\n",
    "    update_time = data[\"data\"][\"lastUpdateTime\"]\n",
    "    areaTree = data[\"data\"][\"areaTree\"]  # 各地方数据\n",
    "\n",
    "    filepath = \"中国各城市病例数据-new.csv\"\n",
    "    with open(filepath, \"w+\", newline=\"\", encoding='utf_8_sig') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        header = [\n",
    "            \"province\", \"city_name\", \"today_confirm\", \"today_confirmCuts\",\n",
    "            \"total_confirm\", \"total_dead\", \"total_heal\", \"total_nowConfirm\",\n",
    "            \"total_suspect\", \"update_time\"]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        china_data = areaTree[0][\"children\"]  # 中国数据\n",
    "        for j in range(len(china_data)):\n",
    "            province = china_data[j][\"name\"]  # 省份\n",
    "            city_list = china_data[j][\"children\"]  # 该省份下面城市列表\n",
    "            for k in range(len(city_list)):\n",
    "                city_name = city_list[k][\"name\"]  # 城市名称\n",
    "                today_confirm = city_list[k][\"today\"][\"confirm\"]  # 今日确认病例\n",
    "                today_confirmCuts = city_list[k][\"today\"][\"confirmCuts\"]\n",
    "                total_confirm = city_list[k][\"total\"][\"confirm\"]  # 总确认病例\n",
    "                total_dead = city_list[k][\"total\"][\"dead\"]  # 总死亡病例\n",
    "                total_heal = city_list[k][\"total\"][\"heal\"]  # 总治愈病例\n",
    "                total_nowConfirm = city_list[k][\"total\"][\"nowConfirm\"]\n",
    "                total_suspect = city_list[k][\"total\"][\"suspect\"]  # 总疑似病例\n",
    "\n",
    "                data_row3 = [\n",
    "                    province, city_name, today_confirm, today_confirmCuts,\n",
    "                    total_confirm, total_dead, total_heal, total_nowConfirm,\n",
    "                    total_suspect, update_time]\n",
    "                writer.writerow(data_row3)\n",
    "\n",
    "\n",
    "url = \"https://view.inews.qq.com/g2/getOnsInfo?{0}&{1}\"\n",
    "name_china = \"name=disease_h5\"\n",
    "callback_china = \"callback=jQuery34105039333360681013_1584838849613&_=1584838849614\"\n",
    "code_china = \"jQuery34105039333360681013_1584838849613\"\n",
    "url_china = url.format(name_china, callback_china)\n",
    "get_china_data(get_json(url_china, code_china))\n",
    "print(\"已完成中国数据的爬取\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
